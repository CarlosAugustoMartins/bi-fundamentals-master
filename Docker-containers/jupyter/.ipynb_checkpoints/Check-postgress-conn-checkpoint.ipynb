{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/51772350/how-to-specify-driver-class-path-when-using-pyspark-within-a-jupyter-notebook\n",
    "\n",
    "#Create SparkSession\n",
    "scSpark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"dvdrental ingestion\")\\\n",
    "        .config('spark.driver.extraClassPath',\n",
    "            \"postgresql-42.2.14.jar\")\\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- store_id: short (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- address_id: short (nullable = true)\n",
      " |-- activebool: boolean (nullable = true)\n",
      " |-- create_date: date (nullable = true)\n",
      " |-- last_update: timestamp (nullable = true)\n",
      " |-- active: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = scSpark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://source-db-container:5432/dvdrental\") \\\n",
    "    .option(\"dbtable\", \"customer\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"postgres\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------------+--------------------+\n",
      "|actor_id|first_name|   last_name|         last_update|\n",
      "+--------+----------+------------+--------------------+\n",
      "|       1|  Penelope|     Guiness|2013-05-26 14:47:...|\n",
      "|       2|      Nick|    Wahlberg|2013-05-26 14:47:...|\n",
      "|       3|        Ed|       Chase|2013-05-26 14:47:...|\n",
      "|       4|  Jennifer|       Davis|2013-05-26 14:47:...|\n",
      "|       5|    Johnny|Lollobrigida|2013-05-26 14:47:...|\n",
      "|       6|     Bette|   Nicholson|2013-05-26 14:47:...|\n",
      "|       7|     Grace|      Mostel|2013-05-26 14:47:...|\n",
      "|       8|   Matthew|   Johansson|2013-05-26 14:47:...|\n",
      "|       9|       Joe|       Swank|2013-05-26 14:47:...|\n",
      "|      10| Christian|       Gable|2013-05-26 14:47:...|\n",
      "|      11|      Zero|        Cage|2013-05-26 14:47:...|\n",
      "|      12|      Karl|       Berry|2013-05-26 14:47:...|\n",
      "|      13|       Uma|        Wood|2013-05-26 14:47:...|\n",
      "|      14|    Vivien|      Bergen|2013-05-26 14:47:...|\n",
      "|      15|      Cuba|     Olivier|2013-05-26 14:47:...|\n",
      "|      16|      Fred|     Costner|2013-05-26 14:47:...|\n",
      "|      17|     Helen|      Voight|2013-05-26 14:47:...|\n",
      "|      18|       Dan|        Torn|2013-05-26 14:47:...|\n",
      "|      19|       Bob|     Fawcett|2013-05-26 14:47:...|\n",
      "|      20|   Lucille|       Tracy|2013-05-26 14:47:...|\n",
      "+--------+----------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jardrv = \"~/drivers/postgresql-42.2.14.jar\"\n",
    "spark = SparkSession.builder.config('spark.driver.extraClassPath', jardrv).getOrCreate()\n",
    "url = 'jdbc:postgresql://source-db-container:5432/dvdrental'\n",
    "properties = {'user': 'postgres', 'password': 'postgres'}\n",
    "df = spark.read.jdbc(url=url, table='actor', properties=properties)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print (type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7f5994317a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.groupBy('actor_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea una tabla nueva\n",
    "url_target = 'jdbc:postgresql://dest-db-container:5432/dvdrental_dwh'\n",
    "properties = {'user': 'postgres', 'password': 'postgres'}\n",
    "\n",
    "df.filter(df.first_name == \"Bob\").write.mode('overwrite').jdbc(url=url_target, table=\"public.actor2\",properties=properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+---------+--------------------+----------+----------+-----------+--------------------+------+\n",
      "|customer_id|store_id|first_name|last_name|               email|address_id|activebool|create_date|         last_update|active|\n",
      "+-----------+--------+----------+---------+--------------------+----------+----------+-----------+--------------------+------+\n",
      "|        524|       1|     Jared|      Ely|jared.ely@sakilac...|       530|      true| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|          1|       1|      Mary|    Smith|mary.smith@sakila...|         5|      true| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|          2|       1|  Patricia|  Johnson|patricia.johnson@...|         6|      true| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|          3|       1|     Linda| Williams|linda.williams@sa...|         7|      true| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|          4|       2|   Barbara|    Jones|barbara.jones@sak...|         8|      true| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|          5|       1| Elizabeth|    Brown|elizabeth.brown@s...|         9|      true| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|          6|       2|  Jennifer|    Davis|jennifer.davis@sa...|        10|      true| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|          7|       1|     Maria|   Miller|maria.miller@saki...|        11|      true| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|          8|       2|     Susan|   Wilson|susan.wilson@saki...|        12|      true| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|          9|       2|  Margaret|    Moore|margaret.moore@sa...|        13|      true| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|         10|       1|   Dorothy|   Taylor|dorothy.taylor@sa...|        14|      true| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|         11|       2|      Lisa| Anderson|lisa.anderson@sak...|        15|      true| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|         12|       1|     Nancy|   Thomas|nancy.thomas@saki...|        16|      true| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|         13|       2|     Karen|  Jackson|karen.jackson@sak...|        17|      true| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|         14|       2|     Betty|    White|betty.white@sakil...|        18|      true| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|         15|       1|     Helen|   Harris|helen.harris@saki...|        19|      true| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|         16|       2|    Sandra|   Martin|sandra.martin@sak...|        20|      true| 2006-02-14|2013-05-26 14:49:...|     0|\n",
      "|         17|       1|     Donna| Thompson|donna.thompson@sa...|        21|      true| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|         18|       2|     Carol|   Garcia|carol.garcia@saki...|        22|      true| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|         19|       1|      Ruth| Martinez|ruth.martinez@sak...|        23|      true| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "+-----------+--------+----------+---------+--------------------+----------+----------+-----------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Probar SERIAL columns\n",
    "jardrv = \"~/drivers/postgresql-42.2.14.jar\"\n",
    "spark = SparkSession.builder.config('spark.driver.extraClassPath', jardrv).getOrCreate()\n",
    "url = 'jdbc:postgresql://source-db-container:5432/dvdrental'\n",
    "properties = {'user': 'postgres', 'password': 'postgres'}\n",
    "df_customer = spark.read.jdbc(url=url, table='customer', properties=properties)\n",
    "df_customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEte registros a una tabla existente\n",
    "url_target = 'jdbc:postgresql://dest-db-container:5432/dvdrental_dwh'\n",
    "properties = {'user': 'postgres', 'password': 'postgres'}\n",
    "\n",
    "#quitar una columna\n",
    "df_customer= df_customer.drop(\"address_id\")\n",
    "\n",
    "df_customer.filter(df_customer.store_id == \"1\").write.mode('append').jdbc(url=url_target, table=\"datalake_raw.customer_dim\",properties=properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEte registros a una tabla existente en STaging para ELT\n",
    "url_target = 'jdbc:postgresql://dest-db-container:5432/dvdrental_staging'\n",
    "properties = {'user': 'postgres', 'password': 'postgres'}\n",
    "\n",
    "#quitar una columna\n",
    "df_customer= df_customer.drop(\"address_id\")\n",
    "\n",
    "df_customer.filter(df_customer.store_id == \"1\").write.mode('append').jdbc(url=url_target, table=\"public.customer_dim_raw\",properties=properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.0'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
