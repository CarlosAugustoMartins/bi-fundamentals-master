{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Types Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries and set general variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Include jar to avoid error: java.sql.SQLException: No suitable driver\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars file:////home/jovyan/work/postgresql-42.2.14.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create SparkSession\n",
    "scSpark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"dimension_types\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Other option to create the SparkSession\n",
    "jardrv = \"~/drivers/postgresql-42.2.14.jar\"\n",
    "\n",
    "#Create SparkSession\n",
    "scSpark2 = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"dimension_types\")\\\n",
    "        .config('spark.driver.extraClassPath',\n",
    "                jardrv)\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create properties\n",
    "properties={\"user\": \"postgres\", \"password\": \"postgres\", \"driver\":\"org.postgresql.Driver\"}\n",
    "url_source = 'jdbc:postgresql://source-db-container:5432/dvdrental'\n",
    "url_target = 'jdbc:postgresql://dest-db-container:5432/dvdrental_staging'\n",
    "url_dwh_target = 'jdbc:postgresql://dest-db-container:5432/dvdrental_dwh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Dimension: Natural vs Surrogated Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **natural key** a single column or set of columns that uniquely identifies a single record in a table, where the key columns are made up of real data.  When I say “real data” I mean data that has meaning and occurs naturally in the world of data.  A natural key is a column value that has a relationship with the rest of the column values in a given data record.   Here are some examples of natural keys values: Social Security Number, ISBN, and TaxId.\n",
    "\n",
    "A **surrogate** keys  don’t have a natural relationship with the rest of the columns in a table.  The surrogate key is just a value that is generated and then stored with the rest of the columns in a record.  The key value is typically generated at run time right before the record is inserted into a table.   It is sometimes also referred to as a dumb key, because there is no meaning associated with the value.  Surrogate keys are commonly a numeric number. \n",
    " \n",
    "The advantage of natural keys is that they exist already, you don't need to introduce a new \"unnatural\" value to your data schema. However, the disadvantage of natural keys is that because they have business meaning they are effectively coupled to your business: you may need to rework your key when your business requirements change.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dimension](img/dimension_structure.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Let's read the customer table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- store_id: short (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- address_id: short (nullable = true)\n",
      " |-- activebool: boolean (nullable = true)\n",
      " |-- create_date: date (nullable = true)\n",
      " |-- last_update: timestamp (nullable = true)\n",
      " |-- active: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jdbc_customer = scSpark.read \\\n",
    "    .jdbc(url_source, \n",
    "         \"public.customer\",\n",
    "         properties=properties)\n",
    "\n",
    "# let's check the schema\n",
    "jdbc_customer.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create temp table for use spark sql\n",
    "jdbc_customer.createOrReplaceTempView(\"source_customer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's include a new column for data load time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use sql in the SparkSession\n",
    "customer_target = scSpark.sql(\"select *,now() as load_date  from source_customer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write info into staging area** \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do it 4 times to duplicate data\n",
    "for i in range (0,3):\n",
    "    customer_target.write \\\n",
    "        .mode('append') \\\n",
    "        .jdbc(url=url_target, table=\"public.stg_dim_customer\",properties=properties)\n",
    "    \n",
    "    #wait 3 sec for generating diff load_date\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As the Natural Key starts in 1, let's create a table with a Surrogate Key**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-- Run this in DB to create a new table with a fake surrogate key in dvdrental_staging DB\n",
    "CREATE TABLE AS public.stg_dim_customer_surrogate AS\n",
    "SELECT row_number () over (order by load_date , customer_id ) as surrogate_key,\n",
    "customer_id, store_id, first_name, last_name, email, address_id, activebool, create_date, last_update, active, load_date\n",
    "FROM public.stg_dim_customer;\n",
    "\n",
    "select *\n",
    "  from public.stg_dim_customer_surrogate\n",
    " order by customer_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role Playing Dimension: Date and Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single physical dimension can be referenced multiple times in a fact table, with each reference linking to a logically distinct role for the dimension. . This is most commonly seen in dimensions such as Time and Customer. We can see that also in city/province/country dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![role playing](img/role_playing.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Date Dimension in DWH**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "create table public.dim_date AS\n",
    "SELECT\t\n",
    "\tto_char(datum,'YYYYMMDD') as Date,\n",
    "\textract(year from datum) AS Year,\n",
    "\textract(month from datum) AS Month,\n",
    "\t-- Localized month name\n",
    "\tto_char(datum, 'TMMonth') AS MonthName,\n",
    "\textract(day from datum) AS Day,\n",
    "\textract(doy from datum) AS DayOfYear,\n",
    "\t-- Localized weekday\n",
    "\tto_char(datum, 'TMDay') AS WeekdayName,\n",
    "\t-- ISO calendar week\n",
    "\textract(week from datum) AS CalendarWeek,\n",
    "\tto_char(datum, 'dd. mm. yyyy') AS FormattedDate,\n",
    "\t'Q' || to_char(datum, 'Q') AS Quartal,\n",
    "\tto_char(datum, 'yyyy/\"Q\"Q') AS YearQuartal,\n",
    "\tto_char(datum, 'yyyy/mm') AS YearMonth,\n",
    "\t-- ISO calendar year and week\n",
    "\tto_char(datum, 'iyyy/IW') AS YearCalendarWeek,\n",
    "\t-- Weekend\n",
    "\tCASE WHEN extract(isodow from datum) in (6, 7) THEN 'Weekend' ELSE 'Weekday' END AS Weekend,\n",
    "\t-- Fixed holidays \n",
    "        -- for America\n",
    "        CASE WHEN to_char(datum, 'MMDD') IN ('0101', '0704', '1225', '1226')\n",
    "\t\tTHEN 'Holiday' ELSE 'No holiday' END\n",
    "\t\tAS AmericanHoliday,\n",
    "        -- for Austria\n",
    "\tCASE WHEN to_char(datum, 'MMDD') IN \n",
    "\t\t('0101', '0106', '0501', '0815', '1101', '1208', '1225', '1226') \n",
    "\t\tTHEN 'Holiday' ELSE 'No holiday' END \n",
    "\t\tAS AustrianHoliday,\n",
    "        -- for Canada\n",
    "        CASE WHEN to_char(datum, 'MMDD') IN ('0101', '0701', '1225', '1226')\n",
    "\t\tTHEN 'Holiday' ELSE 'No holiday' END \n",
    "\t\tAS CanadianHoliday,\n",
    "\t-- Some periods of the year, adjust for your organisation and country\n",
    "\tCASE WHEN to_char(datum, 'MMDD') BETWEEN '0701' AND '0831' THEN 'Summer break'\n",
    "\t     WHEN to_char(datum, 'MMDD') BETWEEN '1115' AND '1225' THEN 'Christmas season'\n",
    "\t     WHEN to_char(datum, 'MMDD') > '1225' OR to_char(datum, 'MMDD') <= '0106' THEN 'Winter break'\n",
    "\t\tELSE 'Normal' END\n",
    "\t\tAS Period,\n",
    "\t-- ISO start and end of the week of this date\n",
    "\tdatum + (1 - extract(isodow from datum))::integer AS CWStart,\n",
    "\tdatum + (7 - extract(isodow from datum))::integer AS CWEnd,\n",
    "\t-- Start and end of the month of this date\n",
    "\tdatum + (1 - extract(day from datum))::integer AS MonthStart,\n",
    "\t(datum + (1 - extract(day from datum))::integer + '1 month'::interval)::date - '1 day'::interval AS MonthEnd\n",
    "FROM (\t\n",
    "\tSELECT '2000-01-01'::DATE + sequence.day AS datum\n",
    "\tFROM generate_series(0,7652) AS sequence(day)\n",
    "\tGROUP BY sequence.day\n",
    "     ) DQ\n",
    "order by 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Time Dimension in DWH**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "create table public.dim_time AS\n",
    "select to_char(minute, 'hh24:mi') AS TimeOfDay,\n",
    "\t-- Hour of the day (0 - 23)\n",
    "\textract(hour from minute) as Hour, \n",
    "\t-- Extract and format quarter hours\n",
    "\tto_char(minute - (extract(minute from minute)::integer % 15 || 'minutes')::interval, 'hh24:mi') ||\n",
    "\t' – ' ||\n",
    "\tto_char(minute - (extract(minute from minute)::integer % 15 || 'minutes')::interval + '14 minutes'::interval, 'hh24:mi')\n",
    "\t\tas QuarterHour,\n",
    "\t-- Minute of the day (0 - 1439)\n",
    "\textract(hour from minute)*60 + extract(minute from minute) as minute,\n",
    "\t-- Names of day periods\n",
    "\tcase when to_char(minute, 'hh24:mi') between '06:00' and '08:29'\n",
    "\t\tthen 'Morning'\n",
    "\t     when to_char(minute, 'hh24:mi') between '08:30' and '11:59'\n",
    "\t\tthen 'AM'\n",
    "\t     when to_char(minute, 'hh24:mi') between '12:00' and '17:59'\n",
    "\t\tthen 'PM'\n",
    "\t     when to_char(minute, 'hh24:mi') between '18:00' and '22:29'\n",
    "\t\tthen 'Evening'\n",
    "\t     else 'Night'\n",
    "\tend as DaytimeName,\n",
    "\t-- Indicator of day or night\n",
    "\tcase when to_char(minute, 'hh24:mi') between '07:00' and '19:59' then 'Day'\n",
    "\t     else 'Night'\n",
    "\tend AS DayNight\n",
    "from (SELECT '0:00'::time + (sequence.minute || ' minutes')::interval AS minute\n",
    "\tFROM generate_series(0,1439) AS sequence(minute)\n",
    "\tGROUP BY sequence.minute\n",
    "     ) DQ\n",
    "order by 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junk Dimension "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To understand **junk dimensions** let's define **cardinality**. This is the number of different values in a table. A low-cardinality shows *few distinct values* (many rows for each value), and a high-cardinality means a *lot of different values* (few rows for each value)\n",
    "\n",
    "A **junk dimension** combines *several low-cardinality flags and attributes into a single dimension table* rather than modeling them as separate dimensions. There are good reasons to create this combined dimension, including reducing the size of the fact table and making the dimensional model easier to work with. \n",
    "\n",
    "**Centipede fact tables** also result when designers embed numerous foreign keys to individual low-cardinality dimension tables rather than creating a junk dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load `category`, `language` y `film`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read category\n",
    "jdbc_category = scSpark.read \\\n",
    "    .jdbc(url_source, \n",
    "         \"public.category\",\n",
    "         properties=properties)\n",
    "\n",
    "# Create temp table for using spark sql\n",
    "jdbc_category.createOrReplaceTempView(\"source_category\")\n",
    "category_target = scSpark.sql(\"select *,now() as load_date  from source_category\")\n",
    "\n",
    "# write category\n",
    "for i in range (1,4):\n",
    "    category_target.write \\\n",
    "            .mode('append') \\\n",
    "            .jdbc(url=url_target, table=\"public.stg_dim_category\",properties=properties)\n",
    "    \n",
    "    #wait 2 sec for enerating diff load_date\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read language\n",
    "jdbc_language = scSpark.read \\\n",
    "    .jdbc(url_source, \n",
    "         \"public.language\",\n",
    "         properties=properties)\n",
    "\n",
    "# Create temp table for using spark sql\n",
    "jdbc_language.createOrReplaceTempView(\"source_language\")\n",
    "language_target = scSpark.sql(\"select *,now() as load_date  from source_language\")\n",
    "\n",
    "# write category\n",
    "for i in range (1,3):\n",
    "    language_target.write \\\n",
    "            .mode('append') \\\n",
    "            .jdbc(url=url_target, table=\"public.stg_dim_language\",properties=properties)\n",
    "    \n",
    "    #wait 2 sec for enerating diff load_date\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read film\n",
    "jdbc_film = scSpark.read \\\n",
    "    .jdbc(url_source, \n",
    "         \"public.film\",\n",
    "         properties=properties)\n",
    "\n",
    "# Create temp table for using spark sql\n",
    "jdbc_film.createOrReplaceTempView(\"source_film\")\n",
    "film_target = scSpark.sql(\"select *,now() as load_date  from source_film\")\n",
    "\n",
    "# write category\n",
    "for i in range (0,3):\n",
    "    film_target.write \\\n",
    "            .mode('append') \\\n",
    "            .jdbc(url=url_target, table=\"public.stg_dim_film\",properties=properties)\n",
    "    \n",
    "    #wait 2 sec for enerating diff load_date\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read film_category\n",
    "jdbc_film_category = scSpark.read \\\n",
    "    .jdbc(url_source, \n",
    "         \"public.film_category\",\n",
    "         properties=properties)\n",
    "\n",
    "# Create temp table for using spark sql\n",
    "jdbc_film_category.createOrReplaceTempView(\"source_film_category\")\n",
    "film_category_target = scSpark.sql(\"select *,now() as load_date  from source_film_category\")\n",
    "\n",
    "# write category\n",
    "for i in range (0,3):\n",
    "    film_category_target.write \\\n",
    "            .mode('append') \\\n",
    "            .jdbc(url=url_target, table=\"public.stg_dim_film_category\",properties=properties)\n",
    "    \n",
    "    #wait 2 sec for enerating diff load_date\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scSpark.sql(\"SELECT  fc.film_id, f.title, f.description, c.name as category_name , l.name as language_name, \\\n",
    "                     f.rental_duration, f.rental_rate, f.length, f.replacement_cost, f.rating, \\\n",
    "                     f.release_year, f.special_features, f.fulltext, now() as load_date \\\n",
    "               FROM source_film f \\\n",
    "               JOIN source_language l \\\n",
    "                 ON f.language_id = l.language_id \\\n",
    "               JOIN source_film_category fc \\\n",
    "                 ON fc.film_id = f.film_id \\\n",
    "               JOIN source_category c \\\n",
    "                 ON c.category_id = fc.category_id\") \\\n",
    "            .write \\\n",
    "            .mode('append') \\\n",
    "            .jdbc(url=url_target, table=\"public.stg_dim_junk_film\",properties=properties)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conformed Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conformed dimensions** are dimensions that are shared by multiple stars. They are used to compare the measures from each star schema.\n",
    "\n",
    "Conformed dimensions are those dimensions which have been designed in such a way that the dimension can be used across many fact tables in different subject areas of the warehouse. It is imperative that the designer plan for these dimensions as they will provide reporting consistency across subject areas and reduce the development costs of those subject areas via reuse of existing dimensions. The date dimension is an excellent example of a conformed dimension. Most warehouses only have a single date dimension used throughout the warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![conformed](img/conformed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create customer tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_csv for tweets\n",
    "df_tweets = scSpark.read.format('csv').options(header= 'true').load('tweets.csv')\n",
    "\n",
    "# Create temp table for using spark sql\n",
    "df_tweets.createOrReplaceTempView(\"df_tweets\")\n",
    "\n",
    "# Modify fields\n",
    "tweets_target = scSpark.sql(\" SELECT CAST(tweet_id AS STRING) tweet_id , sentiment, sentiment_confidence,\\\n",
    "                               negativereason_confidence, name, retweet_count, \\\n",
    "                               tweet_coord, tweet_created, user_timezone, now() as load_date  from df_tweets\")\n",
    "\n",
    "# write to table\n",
    "tweets_target.write \\\n",
    "            .mode('append') \\\n",
    "            .jdbc(url=url_target, table=\"public.fact_tweets\",properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_csv for conformed_customer\n",
    "df_conformed_customer = scSpark.read.format('csv').options(header= 'true').load('conformed_customer.csv')\n",
    "\n",
    "# write to table\n",
    "df_conformed_customer.write \\\n",
    "            .mode('append') \\\n",
    "            .jdbc(url=url_target, table=\"public.conformed_customer\",properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This query returns the number of tweets by sentiment that every user with rentals has made in Twitter"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-- run this query in DB dvdrental_staging \n",
    "\n",
    "\n",
    "with rental_customer as \n",
    "  (select row_number() over (partition by customer_id  order by load_date desc) as load_number,\n",
    "    *\n",
    "    from public.stg_dim_customer sdc     \n",
    "    ) \n",
    "select cc.rental_customer_id , cc.twitter_id ,\n",
    "       rc.first_name, rc.last_name, sentiment, count(1)\n",
    "  from public.conformed_customer cc \n",
    "  join rental_customer rc\n",
    "    on cast (cc.rental_customer_id as int)= rc.customer_id\n",
    "  join public.fact_tweets ft \n",
    "    on trim(ft.\"name\") = trim(cc.twitter_id )\n",
    "  where rc.load_number = 1\n",
    " group by cc.customer_conformed_id , cc.rental_customer_id , cc.twitter_id ,\n",
    "       rc.first_name, rc.last_name, sentiment\n",
    " order by cc.twitter_id ,ft.sentiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slow Changing Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Slowly Changing Dimension (SCD)** is a dimension that stores and manages both current and historical data over time in a data warehouse. \n",
    "\n",
    "There are three types of SCDs and you can use \n",
    "\n",
    "#### What are the three types of SCDs?\n",
    "\n",
    "The three types of SCDs are:\n",
    "\n",
    "##### Type 1 SCDs - Overwriting\n",
    "\n",
    "In a Type 1 SCD the new data overwrites the existing data. Thus the existing data is lost as it is not stored anywhere else. This is the default type of dimension you create. You do not need to specify any additional information to create a Type 1 SCD.\n",
    "\n",
    "##### Type 2 SCDs - Creating another dimension record\n",
    "\n",
    "A Type 2 SCD retains the full history of values. When the value of a chosen attribute changes, the current record is closed. A new record is created with the changed data values and this new record becomes the current record. Each record contains the effective time and expiration time to identify the time period between which the record was active.\n",
    "\n",
    "##### Type 3 SCDs - Creating a current value field\n",
    "\n",
    "A Type 3 SCD stores two versions of values for certain selected level attributes. Each record stores the previous value and the current value of the selected attribute. When the value of any of the selected attributes changes, the current value is stored as the old value and the new value becomes the current value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create an SCD2 dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time to load for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we are merge address and customer, I need to load address to staging db\n",
    "\n",
    "# read address\n",
    "jdbc_address = scSpark.read \\\n",
    "    .jdbc(url_source, \n",
    "         \"public.address\",\n",
    "         properties=properties)\n",
    "\n",
    "# Create temp table for using spark sql\n",
    "jdbc_address.createOrReplaceTempView(\"source_address\")\n",
    "address_target = scSpark.sql(\"select *,now() as load_date  from source_address\")\n",
    "\n",
    "# write category\n",
    "for i in range (1,4):\n",
    "    address_target.write \\\n",
    "            .mode('append') \\\n",
    "            .jdbc(url=url_target, table=\"public.stg_dim_address\",properties=properties)\n",
    "    \n",
    "    #wait 2 sec for enerating diff load_date\n",
    "    time.sleep(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------+---------+-----------------------------------+----------+-----------------+--------+----------+-----------+------------+-----------+-----------------------+------+----------+----------+----------+\n",
      "|customer_dim_id|customer_id|first_name|last_name|email                              |activebool|address          |address2|district  |postal_code|phone       |create_date|last_update            |active|valid_from|valid_to  |dim_active|\n",
      "+---------------+-----------+----------+---------+-----------------------------------+----------+-----------------+--------+----------+-----------+------------+-----------+-----------------------+------+----------+----------+----------+\n",
      "|1              |1          |Mary      |Smith    |mary.smith@sakilacustomer.org      |true      |1913 Hanoi Way   |        |Nagasaki  |35200      |28303384290 |2006-02-14 |2013-05-26 14:49:45.738|1     |2013-05-26|9999-12-31|1         |\n",
      "|2              |2          |Patricia  |Johnson  |patricia.johnson@sakilacustomer.org|true      |1121 Loja Avenue |        |California|17886      |838635286649|2006-02-14 |2013-05-26 14:49:45.738|1     |2013-05-26|9999-12-31|1         |\n",
      "|3              |3          |Linda     |Williams |linda.williams@sakilacustomer.org  |true      |692 Joliet Street|        |Attika    |83579      |448477190408|2006-02-14 |2013-05-26 14:49:45.738|1     |2013-05-26|9999-12-31|1         |\n",
      "|4              |4          |Barbara   |Jones    |barbara.jones@sakilacustomer.org   |true      |1566 Inegl Manor |        |Mandalay  |53561      |705814003527|2006-02-14 |2013-05-26 14:49:45.738|1     |2013-05-26|9999-12-31|1         |\n",
      "|5              |5          |Elizabeth |Brown    |elizabeth.brown@sakilacustomer.org |true      |53 Idfu Parkway  |        |Nantou    |42399      |10655648674 |2006-02-14 |2013-05-26 14:49:45.738|1     |2013-05-26|9999-12-31|1         |\n",
      "+---------------+-----------+----------+---------+-----------------------------------+----------+-----------------+--------+----------+-----------+------------+-----------+-----------------------+------+----------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### get recent_staging_records #####\n",
    "\n",
    "# read customer from  staging DB\n",
    "jdbc_staging_customer = scSpark.read \\\n",
    "    .jdbc(url_target, \n",
    "         \"public.stg_dim_customer\",\n",
    "         properties=properties)\n",
    "\n",
    "jdbc_staging_customer.createOrReplaceTempView(\"stg_dim_customer\")\n",
    "\n",
    "# read address from  staging DB\n",
    "jdbc_staging_address = scSpark.read \\\n",
    "    .jdbc(url_target, \n",
    "         \"public.stg_dim_address\",\n",
    "         properties=properties)\n",
    "\n",
    "jdbc_staging_address.createOrReplaceTempView(\"stg_dim_address\")\n",
    "\n",
    "\n",
    "first_customer_records = \"\"\"\n",
    "WITH recent_customer_staging_records AS(\n",
    "SELECT ROW_NUMBER () OVER (PARTITION BY customer_id  ORDER BY load_date DESC) AS row_num,\n",
    "       customer_id, first_name, last_name, email,  activebool, \n",
    "       create_date, last_update, active, load_date, address_id \n",
    "FROM stg_dim_customer\n",
    "),\n",
    "recent_address_staging_records as(\n",
    "SELECT ROW_NUMBER() OVER (PARTITION BY address_id ORDER BY load_date DESC) AS row_num,\n",
    "       address, address2, district, postal_code, phone,address_id \n",
    "FROM stg_dim_address\n",
    ")\n",
    "SELECT ROW_NUMBER() OVER(ORDER BY customer_id ) AS customer_dim_id,customer_id, first_name, last_name, email, activebool,               \n",
    "       address, address2, district, postal_code, phone,\n",
    "       create_date, last_update, active ,\n",
    "       COALESCE (DATE(last_update) , create_date) AS valid_from,\n",
    "       DATE('9999-12-31') AS valid_to,\n",
    "       1 AS dim_active\n",
    "FROM recent_customer_staging_records c \n",
    "JOIN recent_address_staging_records a\n",
    "  ON c.address_id = a.address_id\n",
    "WHERE a.row_num = 1\n",
    "  AND c.row_num = 1\n",
    "\"\"\"\n",
    "\n",
    "df_first_customer_records = scSpark.sql(first_customer_records)\n",
    "\n",
    "df_first_customer_records.write \\\n",
    "            .mode('overwrite') \\\n",
    "            .jdbc(url=url_dwh_target, table=\"public.dim_customer\",properties=properties)\n",
    "\n",
    "df_first_customer_records.createOrReplaceTempView(\"recent_staging_records\")\n",
    "# ############## review dataset ############## #\n",
    "\n",
    "df_first_customer_records.orderBy(\"customer_dim_id\").show(5, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time to play with dimension SCD type 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-- Change one record and insert another one\n",
    "select *\n",
    "  from public.customer\n",
    "  where customer_id = 5;\n",
    "  \n",
    "-- insert into Dvdrental DB\n",
    "update public.customer\n",
    "   set last_name = 'Brown-Vazquez'\n",
    " where customer_id = 5\n",
    "\n",
    "-- create new customer\n",
    " insert into public.customer (store_id ,first_name ,last_name ,email ,address_id , activebool ,active )\n",
    " values (1,'Luka','Doncic','luka@gmail.com',23,true,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to load again the information to staging DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+-------------+--------------------+----------+----------+-----------+--------------------+------+--------------------+\n",
      "|customer_id|store_id|first_name|    last_name|               email|address_id|activebool|create_date|         last_update|active|           load_date|\n",
      "+-----------+--------+----------+-------------+--------------------+----------+----------+-----------+--------------------+------+--------------------+\n",
      "|          5|       1| Elizabeth|Brown-Vazquez|elizabeth.brown@s...|         9|      true| 2006-02-14|2020-12-22 20:23:...|     1|2020-12-23 02:23:...|\n",
      "+-----------+--------+----------+-------------+--------------------+----------+----------+-----------+--------------------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check we have the update in the source DB\n",
    "customer_target.filter(customer_target.customer_id == 5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write the changes into Staging DB\n",
    "\n",
    "customer_target.write \\\n",
    "        .mode('append') \\\n",
    "        .jdbc(url=url_target, table=\"public.stg_dim_customer\",properties=properties)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-- run query in Staging DB\n",
    "SELECT customer_id, store_id, first_name, last_name, email, address_id, activebool, create_date, last_update, active, load_date\n",
    "FROM public.stg_dim_customer\n",
    "where customer_id in (5,600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------+-------------+--------------------+----------+---------------+--------+--------+-----------+-----------+-----------+--------------------+------+----------+----------+----------+\n",
      "|customer_dim_id|customer_id|first_name|    last_name|               email|activebool|        address|address2|district|postal_code|      phone|create_date|         last_update|active|valid_from|  valid_to|dim_active|\n",
      "+---------------+-----------+----------+-------------+--------------------+----------+---------------+--------+--------+-----------+-----------+-----------+--------------------+------+----------+----------+----------+\n",
      "|              5|          5| Elizabeth|Brown-Vazquez|elizabeth.brown@s...|      true|53 Idfu Parkway|        |  Nantou|      42399|10655648674| 2006-02-14|2020-12-22 20:23:...|     1|2020-12-22|9999-12-31|         1|\n",
      "+---------------+-----------+----------+-------------+--------------------+----------+---------------+--------+--------+-----------+-----------+-----------+--------------------+------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let's check we have the new records in STG\n",
    "#df_first_customer_records.filter(df_first_customer_records.customer_id == 600).show()\n",
    "df_first_customer_records.filter(df_first_customer_records.customer_id == 5).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Steps for update a SCD2\n",
    " \n",
    " Once the new record is in STG we need to:\n",
    " \n",
    "1. Create new current records for existing customers - (new Elizabeth Brown-Vazquez)\n",
    "2. Find previous current records to expire - (PK for Elizabeth Brown-Vazquez)\n",
    "3. Expire previous current records - (old Elizabeth Brown-Vazquez)\n",
    "4. Isolate unaffected records - (all the other customers)\n",
    "5. Create records for new customers - (new Luca Doncic)\n",
    "6. Combine the datasets for new SCD2 - Insert into dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------+---------+--------------------+----------+-----------------+--------+----------+-----------+------------+-----------+--------------------+------+----------+----------+----------+\n",
      "|customer_dim_id|customer_id|first_name|last_name|               email|activebool|          address|address2|  district|postal_code|       phone|create_date|         last_update|active|valid_from|  valid_to|dim_active|\n",
      "+---------------+-----------+----------+---------+--------------------+----------+-----------------+--------+----------+-----------+------------+-----------+--------------------+------+----------+----------+----------+\n",
      "|              1|          1|      Mary|    Smith|mary.smith@sakila...|      true|   1913 Hanoi Way|        |  Nagasaki|      35200| 28303384290| 2006-02-14|2013-05-26 14:49:...|     1|2013-05-26|9999-12-31|         1|\n",
      "|              2|          2|  Patricia|  Johnson|patricia.johnson@...|      true| 1121 Loja Avenue|        |California|      17886|838635286649| 2006-02-14|2013-05-26 14:49:...|     1|2013-05-26|9999-12-31|         1|\n",
      "|              3|          3|     Linda| Williams|linda.williams@sa...|      true|692 Joliet Street|        |    Attika|      83579|448477190408| 2006-02-14|2013-05-26 14:49:...|     1|2013-05-26|9999-12-31|         1|\n",
      "|              4|          4|   Barbara|    Jones|barbara.jones@sak...|      true| 1566 Inegl Manor|        |  Mandalay|      53561|705814003527| 2006-02-14|2013-05-26 14:49:...|     1|2013-05-26|9999-12-31|         1|\n",
      "|              5|          5| Elizabeth|    Brown|elizabeth.brown@s...|      true|  53 Idfu Parkway|        |    Nantou|      42399| 10655648674| 2006-02-14|2013-05-26 14:49:...|     1|2013-05-26|9999-12-31|         1|\n",
      "+---------------+-----------+----------+---------+--------------------+----------+-----------------+--------+----------+-----------+------------+-----------+--------------------+------+----------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read SCD2 \n",
    "jdbc_customer_dim = scSpark.read \\\n",
    "    .jdbc(url_dwh_target, \n",
    "         \"public.dim_customer\",\n",
    "         properties=properties)\n",
    "\n",
    "# Create temp table for using spark sql\n",
    "jdbc_customer_dim.createOrReplaceTempView(\"current_scd2\")\n",
    "\n",
    "jdbc_customer_dim.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------+-------------+----------------------------------+----------+---------------+--------+--------+-----------+-----------+-----------+--------------------------+------+----------+----------+----------+\n",
      "|customer_dim_id|customer_id|first_name|last_name    |email                             |activebool|address        |address2|district|postal_code|phone      |create_date|last_update               |active|valid_from|valid_to  |dim_active|\n",
      "+---------------+-----------+----------+-------------+----------------------------------+----------+---------------+--------+--------+-----------+-----------+-----------+--------------------------+------+----------+----------+----------+\n",
      "|5              |5          |Elizabeth |Brown-Vazquez|elizabeth.brown@sakilacustomer.org|true      |53 Idfu Parkway|        |Nantou  |42399      |10655648674|2006-02-14 |2020-12-22 20:23:11.159984|1     |2020-12-22|9999-12-31|1         |\n",
      "+---------------+-----------+----------+-------------+----------------------------------+----------+---------------+--------+--------+-----------+-----------+-----------+--------------------------+------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ############## 1. Create new current records for existing customers ############## #\n",
    "# I should have one new record for Elizabeth Brown-Vazquez, customer_id 5\n",
    "\n",
    "\n",
    "hd_new_curr_recs = \"\"\"\n",
    " SELECT t.customer_dim_id, s.customer_id, s.first_name ,s.last_name, s.email, s.activebool,               \n",
    "        s.address, s.address2, s.district, s.postal_code, s.phone,\n",
    "        s.create_date, s.last_update, s.active ,\n",
    "        COALESCE (DATE(s.last_update) , DATE(s.create_date)) AS valid_from,\n",
    "        DATE('9999-12-31') AS valid_to,\n",
    "        1 AS dim_active        \n",
    " FROM     recent_staging_records s\n",
    "          INNER JOIN current_scd2 t\n",
    "              ON t.customer_id = s.customer_id\n",
    "              AND CAST(t.dim_active AS STRING) = '1'\n",
    " WHERE   NVL(s.first_name, '') <> NVL(t.first_name, '')\n",
    "        OR NVL(s.last_name, '') <> NVL(t.last_name, '')\n",
    "        OR NVL(s.email, '') <> NVL(t.email, '')          \n",
    "        OR NVL(s.address, '') <> NVL(t.address, '')\n",
    "        OR NVL(s.address2, '') <> NVL(t.address2, '')\n",
    "        OR NVL(s.district, '') <> NVL(t.district, '')\n",
    "        OR NVL(s.postal_code, '') <> NVL(t.postal_code, '')\n",
    "        OR NVL(s.phone, '') <> NVL(t.phone, '')\n",
    "\"\"\"\n",
    "df_new_curr_recs = scSpark.sql(hd_new_curr_recs)\n",
    "\n",
    "df_new_curr_recs.createOrReplaceTempView(\"new_curr_recs\")\n",
    "# ############## review dataset ############## #\n",
    "\n",
    "df_new_curr_recs.orderBy(\"customer_id\").show(3, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|customer_dim_id|\n",
      "+---------------+\n",
      "|              5|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ########### 2. Find previous current records to expire  ########\n",
    "# ########### isolate keys of records to be modified ########### #\n",
    "df_modfied_keys = df_new_curr_recs.select(\"customer_dim_id\")\n",
    "df_modfied_keys.createOrReplaceTempView(\"modfied_keys\")\n",
    "df_modfied_keys.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------+---------+----------------------------------+----------+---------------+--------+--------+-----------+-----------+-----------+-----------------------+------+----------+----------+----------+\n",
      "|customer_dim_id|customer_id|first_name|last_name|email                             |activebool|address        |address2|district|postal_code|phone      |create_date|last_update            |active|valid_from|valid_to  |dim_active|\n",
      "+---------------+-----------+----------+---------+----------------------------------+----------+---------------+--------+--------+-----------+-----------+-----------+-----------------------+------+----------+----------+----------+\n",
      "|5              |5          |Elizabeth |Brown    |elizabeth.brown@sakilacustomer.org|true      |53 Idfu Parkway|        |Nantou  |42399      |10655648674|2006-02-14 |2013-05-26 14:49:45.738|1     |2013-05-26|2020-12-22|0         |\n",
      "+---------------+-----------+----------+---------+----------------------------------+----------+---------------+--------+--------+-----------+-----------+-----------+-----------------------+------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ############## 3. Expire previous current records  ############\n",
    "# ############## create new hist recs dataset ############## #\n",
    "# we should have the modified register as expired\n",
    "\n",
    "hd_new_hist_recs = \"\"\"\n",
    " SELECT   t.customer_dim_id, t.customer_id, t.first_name, t.last_name, \n",
    "          t.email, t.activebool, t.address, t.address2, t.district, \n",
    "          t.postal_code, t.phone, t.create_date, t.last_update, t.active, \n",
    "          DATE(t.valid_from) AS valid_from ,\n",
    "          DATE(FROM_UTC_TIMESTAMP(CURRENT_TIMESTAMP, 'EST')) AS valid_to,\n",
    "          '0' AS dim_active  -- here we expire\n",
    " FROM     current_scd2 t\n",
    "          INNER JOIN modfied_keys k\n",
    "              ON k.customer_dim_id = t.customer_dim_id\n",
    " WHERE    t.dim_active = '1'\n",
    "\"\"\"\n",
    "df_new_hist_recs = scSpark.sql(hd_new_hist_recs)\n",
    "\n",
    "df_new_hist_recs.createOrReplaceTempView(\"new_hist_recs\")\n",
    "\n",
    "# ############## review dataset ############## #\n",
    "df_new_hist_recs.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------+---------+-----------------------------------+----------+----------------------------------+--------+--------------+-----------+------------+-----------+-----------------------+------+----------+----------+----------+\n",
      "|customer_dim_id|customer_id|first_name|last_name|email                              |activebool|address                           |address2|district      |postal_code|phone       |create_date|last_update            |active|valid_from|valid_to  |dim_active|\n",
      "+---------------+-----------+----------+---------+-----------------------------------+----------+----------------------------------+--------+--------------+-----------+------------+-----------+-----------------------+------+----------+----------+----------+\n",
      "|1              |1          |Mary      |Smith    |mary.smith@sakilacustomer.org      |true      |1913 Hanoi Way                    |        |Nagasaki      |35200      |28303384290 |2006-02-14 |2013-05-26 14:49:45.738|1     |2013-05-26|9999-12-31|1         |\n",
      "|2              |2          |Patricia  |Johnson  |patricia.johnson@sakilacustomer.org|true      |1121 Loja Avenue                  |        |California    |17886      |838635286649|2006-02-14 |2013-05-26 14:49:45.738|1     |2013-05-26|9999-12-31|1         |\n",
      "|3              |3          |Linda     |Williams |linda.williams@sakilacustomer.org  |true      |692 Joliet Street                 |        |Attika        |83579      |448477190408|2006-02-14 |2013-05-26 14:49:45.738|1     |2013-05-26|9999-12-31|1         |\n",
      "|4              |4          |Barbara   |Jones    |barbara.jones@sakilacustomer.org   |true      |1566 Inegl Manor                  |        |Mandalay      |53561      |705814003527|2006-02-14 |2013-05-26 14:49:45.738|1     |2013-05-26|9999-12-31|1         |\n",
      "|6              |6          |Jennifer  |Davis    |jennifer.davis@sakilacustomer.org  |true      |1795 Santiago de Compostela Way   |        |Texas         |18743      |860452626434|2006-02-14 |2013-05-26 14:49:45.738|1     |2013-05-26|9999-12-31|1         |\n",
      "|7              |7          |Maria     |Miller   |maria.miller@sakilacustomer.org    |true      |900 Santiago de Compostela Parkway|        |Central Serbia|93896      |716571220373|2006-02-14 |2013-05-26 14:49:45.738|1     |2013-05-26|9999-12-31|1         |\n",
      "+---------------+-----------+----------+---------+-----------------------------------+----------+----------------------------------+--------+--------------+-----------+------------+-----------+-----------------------+------+----------+----------+----------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ############## 4. Isolate unaffected records  ################\n",
    "# ############## create unaffected recs dataset ############## #\n",
    "# row for customer_id 5 should not appear\n",
    "hd_unaffected_recs = \"\"\"\n",
    " SELECT   t.customer_dim_id, t.customer_id, \n",
    "          t.first_name, t.last_name, \n",
    "          t.email, t.activebool, \n",
    "          t.address, t.address2, \n",
    "          t.district, t.postal_code, \n",
    "          t.phone, t.create_date, \n",
    "          t.last_update, t.active, \n",
    "          t.valid_from ,t.valid_to,\n",
    "          t.dim_active\n",
    " FROM     current_scd2 t\n",
    "LEFT JOIN modfied_keys k\n",
    "       ON k.customer_dim_id = t.customer_dim_id\n",
    "    WHERE k.customer_dim_id IS NULL       \n",
    "\"\"\"\n",
    "df_unaffected_recs = scSpark.sql(hd_unaffected_recs)\n",
    "df_unaffected_recs.createOrReplaceTempView(\"unaffected_recs\")\n",
    "\n",
    "# ############## review dataset ############## #\n",
    "df_unaffected_recs.orderBy(\"customer_dim_id\").show(6, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------------+----------+---------------------+--------+-------------+-----------+------------+-----------+--------------------------+------+----------+----------+----------+\n",
      "|customer_id|first_name|last_name|email         |activebool|address              |address2|district     |postal_code|phone       |create_date|last_update               |active|valid_from|valid_to  |dim_active|\n",
      "+-----------+----------+---------+--------------+----------+---------------------+--------+-------------+-----------+------------+-----------+--------------------------+------+----------+----------+----------+\n",
      "|600        |Luka      |Doncic   |luka@gmail.com|true      |1417 Lancaster Avenue|        |Northern Cape|72192      |272572357893|2020-12-22 |2020-12-22 20:23:19.110425|1     |2020-12-22|9999-12-31|1         |\n",
      "+-----------+----------+---------+--------------+----------+---------------------+--------+-------------+-----------+------------+-----------+--------------------------+------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################ 5. Create records for new customers ######\n",
    "# ############## create new recs dataset ############## #\n",
    "# We should have the new record for Luka Doncic\n",
    "hd_new_cust = \"\"\"\n",
    " SELECT s.customer_id, s.first_name, \n",
    "        s.last_name, s.email, \n",
    "        s.activebool,s.address, \n",
    "        s.address2, s.district, \n",
    "        s.postal_code, s.phone,\n",
    "        s.create_date, s.last_update, \n",
    "        s.active ,\n",
    "        COALESCE (DATE(s.last_update) , DATE(s.create_date)) AS valid_from,\n",
    "        DATE('9999-12-31') AS valid_to,\n",
    "        1 AS dim_active        \n",
    " FROM     recent_staging_records s\n",
    "LEFT JOIN current_scd2 t\n",
    "       ON t.customer_id = s.customer_id\n",
    "    WHERE t.customer_id IS NULL\n",
    "\"\"\"\n",
    "df_new_cust = scSpark.sql(hd_new_cust)\n",
    "df_new_cust.createOrReplaceTempView(\"new_cust\")\n",
    "\n",
    "# ############## review dataset ############## #\n",
    "df_new_cust.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599\n"
     ]
    }
   ],
   "source": [
    "# Get max surrogate key\n",
    "v_max_key = scSpark.sql(\n",
    "    \"SELECT STRING(MAX(customer_dim_id)) FROM current_scd2\"\n",
    ").collect()[0][0]\n",
    "\n",
    "print (v_max_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------+-------------+----------------------------------+----------+---------------------+--------+-------------+-----------+------------+-----------+--------------------------+------+----------+----------+----------+\n",
      "|customer_dim_id|customer_id|first_name|last_name    |email                             |activebool|address              |address2|district     |postal_code|phone       |create_date|last_update               |active|valid_from|valid_to  |dim_active|\n",
      "+---------------+-----------+----------+-------------+----------------------------------+----------+---------------------+--------+-------------+-----------+------------+-----------+--------------------------+------+----------+----------+----------+\n",
      "|601            |5          |Elizabeth |Brown-Vazquez|elizabeth.brown@sakilacustomer.org|true      |53 Idfu Parkway      |        |Nantou       |42399      |10655648674 |2006-02-14 |2020-12-22 20:23:11.159984|1     |2020-12-22|9999-12-31|1         |\n",
      "|600            |600        |Luka      |Doncic       |luka@gmail.com                    |true      |1417 Lancaster Avenue|        |Northern Cape|72192      |272572357893|2020-12-22 |2020-12-22 20:23:19.110425|1     |2020-12-22|9999-12-31|1         |\n",
      "|599            |599        |Austin    |Cintron      |austin.cintron@sakilacustomer.org |true      |1325 Fukuyama Street |        |Heilongjiang |27107      |288241215394|2006-02-14 |2013-05-26 14:49:45.738   |1     |2013-05-26|9999-12-31|1         |\n",
      "+---------------+-----------+----------+-------------+----------------------------------+----------+---------------------+--------+-------------+-----------+------------+-----------+--------------------------+------+----------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################ 6. Combine the datasets for new SCD2 #########\n",
    "# ############## create new scd2 dataset ############## #\n",
    "\n",
    "# Query for creating the whole table\n",
    "hd_new_scd2 = \"\"\"\n",
    " WITH a_cte\n",
    " AS   (\n",
    "        SELECT     x.customer_id, x.first_name, \n",
    "                   x.last_name, x.email, \n",
    "                   x.activebool, x.address, \n",
    "                   x.address2, x.district, \n",
    "                   x.postal_code, x.phone,\n",
    "                   x.create_date, x.last_update, \n",
    "                   x.active , x.valid_from,\n",
    "                   x.valid_to , x.dim_active\n",
    "        FROM       new_cust x        \n",
    "        UNION ALL\n",
    "        SELECT     y.customer_id, y.first_name, \n",
    "                   y.last_name, y.email, \n",
    "                   y.activebool, y.address, \n",
    "                   y.address2, y.district, \n",
    "                   y.postal_code, y.phone,\n",
    "                   y.create_date, y.last_update, \n",
    "                   y.active , y.valid_from,\n",
    "                   y.valid_to , y.dim_active\n",
    "        FROM       new_curr_recs y\n",
    "      )\n",
    "  ,   b_cte\n",
    "  AS  (\n",
    "        SELECT  ROW_NUMBER() OVER(ORDER BY a.valid_from)\n",
    "                    + BIGINT('{v_max_key}') AS customer_dim_id,\n",
    "                a.customer_id, a.first_name, \n",
    "                a.last_name, a.email, \n",
    "                a.activebool, a.address, \n",
    "                a.address2, a.district, \n",
    "                a.postal_code, a.phone,\n",
    "                a.create_date, a.last_update, \n",
    "                a.active , a.valid_from,\n",
    "                a.valid_to , a.dim_active\n",
    "        FROM    a_cte a\n",
    "      )\n",
    "  SELECT  customer_dim_id, customer_id, \n",
    "          first_name, \n",
    "          last_name, email, \n",
    "          activebool, address, \n",
    "          address2, district, \n",
    "          postal_code, phone,\n",
    "          create_date, last_update, \n",
    "          active , valid_from, valid_to,\n",
    "          dim_active\n",
    "  FROM    b_cte\n",
    "  UNION ALL\n",
    "  SELECT  customer_dim_id, customer_id, \n",
    "          first_name, \n",
    "          last_name, email, \n",
    "          activebool, address, \n",
    "          address2, district, \n",
    "          postal_code, phone,\n",
    "          create_date, last_update, \n",
    "          active , valid_from,valid_to,\n",
    "          dim_active\n",
    "  FROM    unaffected_recs\n",
    "  UNION ALL\n",
    "  SELECT  customer_dim_id, customer_id, \n",
    "          first_name, \n",
    "          last_name, email, \n",
    "          activebool, address, \n",
    "          address2, district, \n",
    "          postal_code, phone,\n",
    "          create_date, last_update, \n",
    "          active , valid_from, valid_to,\n",
    "          dim_active\n",
    "  FROM    new_hist_recs\n",
    "\"\"\"\n",
    "df_new_scd2 = scSpark.sql(hd_new_scd2.replace(\"{v_max_key}\", v_max_key))\n",
    "\n",
    "#Insert into table\n",
    "#df_new_scd2.write \\\n",
    "#        .mode('overwrite') \\\n",
    "#        .jdbc(url=url_dwh_target, table=\"public.dim_customer\",properties=properties)\n",
    "        \n",
    "#df_new_scd2.coalesce(1).write.csv(\"test_csd2.csv\", header=True)\n",
    "# ############## review dataset ############## #\n",
    "\n",
    "df_new_scd2.orderBy(\"customer_dim_id\",ascending=False).show(3, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a backup just so\n",
    "df_backup_scd2 = df_new_scd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|customer_dim_id|\n",
      "+---------------+\n",
      "|601            |\n",
      "|600            |\n",
      "|599            |\n",
      "+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check backup  \n",
    "df_backup_scd2.select(['customer_dim_id']).orderBy(\"customer_dim_id\",ascending=False).show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test write temporal_scd2 table with backup\n",
    "\n",
    "df_backup_scd2.write \\\n",
    "              .mode(\"overwrite\") \\\n",
    "              .jdbc(url=url_dwh_target, table = \"public.temporal_scd2\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read temporal table\n",
    "df_temporal_scd2 = scSpark.read \\\n",
    "    .jdbc(url_dwh_target, \n",
    "         \"public.temporal_scd2\",\n",
    "         properties=properties)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write dimension with the backup info\n",
    "df_temporal_scd2.write \\\n",
    "            .mode('overwrite') \\\n",
    "            .jdbc(url=url_dwh_target, table=\"public.dim_customer\",properties=properties)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You're done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
